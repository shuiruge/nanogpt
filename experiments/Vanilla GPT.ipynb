{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eRCpWuVeBogx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eRCpWuVeBogx",
    "outputId": "72eab9ac-9e4e-4448-d3f8-aab258e606f6"
   },
   "outputs": [],
   "source": [
    "# # On Google colab:\n",
    "\n",
    "# # Data\n",
    "# !mkdir data\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "# !mv input.txt data/shakespeare.txt\n",
    "\n",
    "# # Module\n",
    "# !pip install git+https://github.com/shuiruge/nanogpt.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25758ae2-c983-4c56-a1e8-8bdb4858f44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locally:\n",
    "import sys\n",
    "sys.path.append('../nanogpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "250aebf8-3872-4ac5-ae23-a6beed98703e",
   "metadata": {
    "id": "250aebf8-3872-4ac5-ae23-a6beed98703e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-17 14:09:15.436270: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Layer, Dense, LayerNormalization, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import AdamW\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.callbacks import EarlyStopping\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "# from nanogpt.utils import (\n",
    "from utils import (\n",
    "    CharacterTokenizer, LanguageModelDataGenerator, TokPosEmbedding, FeedForward,\n",
    "    MultiHeadWrapper, ResNetWrapper, luong_attention,\n",
    ")\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7f6f09f-948d-4430-90a2-ac20b2310ec5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7f6f09f-948d-4430-90a2-ac20b2310ec5",
    "outputId": "6df1579a-bb00-46ae-8aaa-d9ea5c207c59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "with open('data/shakespeare.txt', 'r') as f:\n",
    "    corpus = ''.join(f.readlines())\n",
    "print(corpus[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "563aecf0-7f80-4e8c-a130-57a8bcff1df0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "563aecf0-7f80-4e8c-a130-57a8bcff1df0",
    "outputId": "8fa93e9e-a4a9-45ce-d17b-e38018bf90e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = CharacterTokenizer(corpus)\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "252be36e-effd-4d54-93dc-bf9bf50aee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 64\n",
    "data = LanguageModelDataGenerator(tokenizer.encode(corpus))\n",
    "\n",
    "contexts = []\n",
    "targets = []\n",
    "while True:\n",
    "    try:\n",
    "        context, target = data(seq_len, False)\n",
    "        contexts.append(context)\n",
    "        targets.append(target)\n",
    "    except StopIteration:\n",
    "        break\n",
    "contexts = np.stack(contexts).astype('int64')\n",
    "targets = np.asarray(targets, 'int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24099cff-8787-407e-a831-bf74d35a0c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14,\n",
       "         43, 44, 53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,\n",
       "          1, 39, 52, 63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43,\n",
       "         39, 56,  1, 51, 43,  1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50],\n",
       "        [47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43,\n",
       "         44, 53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1,\n",
       "         39, 52, 63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39,\n",
       "         56,  1, 51, 43,  1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50],\n",
       "        [56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "         53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39,\n",
       "         52, 63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,\n",
       "          1, 51, 43,  1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10]]),\n",
       " array([50, 10,  0]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts[:3], targets[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c494a087-80a2-46de-8f16-79f1ae6e7879",
   "metadata": {
    "id": "c494a087-80a2-46de-8f16-79f1ae6e7879"
   },
   "outputs": [],
   "source": [
    "class VanillaSelfAttention(Layer):\n",
    "    \"\"\"A vanilla version of self-attention.\n",
    "    \n",
    "    It is called vanilla because it has no trainable variables at all! The\n",
    "    function of this kind of self-attention is just communicating between\n",
    "    each token (or each node if there is an image of community in your mind).\n",
    "    So, it has no resposibility for computation, which is completely left to\n",
    "    feed-forward layers.\n",
    "\n",
    "    For each node i with state $x_i$ and each node j with state $x_j$, the\n",
    "    the information propagate from node i to node j is $w_{ij} x_i$, where\n",
    "    $w_{ij}$ is proportional to $\\exp(x_i \\dot x_j)$ and is normalized so that,\n",
    "    for all nodes that sends to node j, the total $\\sum_i w_{ij}$ shall be unit.\n",
    "\n",
    "    In this communication viewpoint, the multi-head means the multi-channel\n",
    "    of communication. Each channel propagates one kind of information. And\n",
    "    different kinds of information are sent to different target nodes.\n",
    "\n",
    "    We mask out the self-communication. This is like a Hopfield network, where\n",
    "    self-interaction is neglected (the weight matrix has a vanished diagonal).\n",
    "\n",
    "    Args:\n",
    "        num_heads: int\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.multihead = MultiHeadWrapper(self.num_heads)\n",
    "\n",
    "    def call(self, x, return_weights=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: tf.Tensor\n",
    "                Shape [..., seq_len, dim]\n",
    "            return_weights: bool\n",
    "                If return the attention weights. Defaults to False.\n",
    "\n",
    "        Returns: tf.Tensor or (tf.Tensor, tf.Tensor)\n",
    "            If return_weights is false, then return the output only, which has\n",
    "            the same shape and dtype as the x. Otherwise, return the output as\n",
    "            well as the attention weights, which has shape\n",
    "            [..., num_heads, seq_len, seq_len].\n",
    "        \"\"\"\n",
    "        # Split the last dimension into multiple heads.\n",
    "        query = self.multihead.split_heads(x)\n",
    "        key = self.multihead.split_heads(x)\n",
    "        value = self.multihead.split_heads(x)\n",
    "\n",
    "        # Mask the self-communication.\n",
    "        seq_len = tf.shape(x)[-2]\n",
    "        mask = tf.linalg.diag(tf.ones([seq_len]))\n",
    "\n",
    "        # The communication is implemented by a Luong-style attention.\n",
    "        output, attention_weights = luong_attention(query, key, value, mask)\n",
    "\n",
    "        # Concatenate the heads together.\n",
    "        output = self.multihead.concat_heads(output)\n",
    "        return (output, attention_weights) if return_weights else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e26b40f9-380f-4309-9e3a-37c8458ad0e8",
   "metadata": {
    "id": "e26b40f9-380f-4309-9e3a-37c8458ad0e8"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int\n",
    "    seq_len: int\n",
    "    embed_dim: int\n",
    "    model_dim: int\n",
    "    num_heads: int\n",
    "    ffd_hidden_units: List[int]\n",
    "    num_trans_blocks: int\n",
    "\n",
    "\n",
    "class VanillaGPT(Model):\n",
    "    \"\"\"Build a vanilla GPT with vanilla self-attention.\"\"\"\n",
    "\n",
    "    def __init__(self, cfg: GPTConfig, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.embedding_layer = TokPosEmbedding(\n",
    "            cfg.vocab_size, cfg.seq_len, cfg.embed_dim)\n",
    "\n",
    "        # The so-called transformer-blocks.\n",
    "        self.trans_blocks = []\n",
    "        for _ in range(cfg.num_trans_blocks):\n",
    "            # As we have discussed in the docstring of VanillaSelfAttention,\n",
    "            # the task of computation is left to feed-forward layers. So,\n",
    "            # for communication:\n",
    "            self.trans_blocks.append(\n",
    "                # ResNetWrapper will introduce a LayerNormalization, which\n",
    "                # is trainable.\n",
    "                ResNetWrapper(VanillaSelfAttention(cfg.num_heads))\n",
    "            )\n",
    "            # and for computation:\n",
    "            self.trans_blocks.append(\n",
    "                ResNetWrapper(FeedForward(cfg.ffd_hidden_units, cfg.model_dim))\n",
    "            )\n",
    "\n",
    "        self.output_layer = Dense(cfg.vocab_size)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding_layer(x)\n",
    "        for layer in self.trans_blocks:\n",
    "            x = layer(x)\n",
    "        # We only use the last sequence element for output.\n",
    "        # It is such a waste!\n",
    "        # And why it is the last one? Why not the others? Maybe some of the\n",
    "        # others out-performs the last.\n",
    "        x = x[:, -1, :]\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def generate(self, init_token_ids, num_new_tokens, T):\n",
    "        \"\"\"Generates new tokens from the initial.\n",
    "\n",
    "        The \"temperature\" T controls the randomness, as in the Boltzmann\n",
    "        distributions.\n",
    "\n",
    "        Args:\n",
    "            init_token_ids: List[int]\n",
    "            num_new_tokens: int\n",
    "            T: float\n",
    "\n",
    "        Returns: List[int]\n",
    "            It also includes the initial token-IDs. So, the length is the\n",
    "            `len(initial_token_ids) + \n",
    "        \"\"\"\n",
    "        init_token_ids = tf.convert_to_tensor(init_token_ids)\n",
    "\n",
    "        # Add batch_size for matching the input shape of `self.call`.\n",
    "        # [1, len(init_token_ids)]\n",
    "        token_ids = tf.expand_dims(init_token_ids, axis=0)\n",
    "\n",
    "        for _ in range(num_new_tokens):\n",
    "            # [1, vocab_size]\n",
    "            logits = self(token_ids[:, -self.cfg.seq_len:])\n",
    "            # [1, 1]\n",
    "            next_token_id = tf.random.categorical(logits/T, 1)\n",
    "            token_ids = tf.concat([token_ids, next_token_id], axis=1)\n",
    "\n",
    "        # Drop the batch_size\n",
    "        token_ids = tf.squeeze(token_ids, axis=0)\n",
    "        return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c963ba25-d689-48e5-a736-6ef1a38446ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c963ba25-d689-48e5-a736-6ef1a38446ba",
    "outputId": "f13adb6b-4e8e-4ea5-b8bb-b3b2aee64727"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vanilla_gpt\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " tok_pos_embedding (TokPosE  multiple                  8256      \n",
      " mbedding)                                                       \n",
      "                                                                 \n",
      " layer_normalization (Layer  multiple                  128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " resnet_wrapper_of_vanilla_  multiple                  0         \n",
      " self_attention (ResNetWrap                                      \n",
      " per)                                                            \n",
      "                                                                 \n",
      " layer_normalization_1 (Lay  multiple                  128       \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " resnet_wrapper_of_feed_for  multiple                  33088     \n",
      " ward (ResNetWrapper)                                            \n",
      "                                                                 \n",
      " layer_normalization_2 (Lay  multiple                  128       \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " resnet_wrapper_of_vanilla_  multiple                  0         \n",
      " self_attention_1 (ResNetWr                                      \n",
      " apper)                                                          \n",
      "                                                                 \n",
      " layer_normalization_3 (Lay  multiple                  128       \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " resnet_wrapper_of_feed_for  multiple                  33088     \n",
      " ward_1 (ResNetWrapper)                                          \n",
      "                                                                 \n",
      " dense_4 (Dense)             multiple                  4225      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79169 (309.25 KB)\n",
      "Trainable params: 79169 (309.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Andrej Karpathy's configuration.\n",
    "# See: https://www.youtube.com/watch?v=kCc8FmEb1nY&t=6068s, at 1:40:30.\n",
    "# This configuration will get a minimal validation loss about 1.48.\n",
    "# And this configuration is not for our vanilla GPT, but Andrej's nano GPT,\n",
    "# which is much more complicated than ours.\n",
    "# cfg = GPTConfig(tokenizer.vocab_size, seq_len,\n",
    "#                 embed_dim=384,\n",
    "#                 model_dim=384,\n",
    "#                 num_heads=6,\n",
    "#                 ffd_hidden_units=[4*384],\n",
    "#                 num_trans_blocks=6)\n",
    "\n",
    "# cfg = GPTConfig(tokenizer.vocab_size, seq_len,\n",
    "#                 embed_dim=64,\n",
    "#                 model_dim=64,\n",
    "#                 num_heads=4,\n",
    "#                 ffd_hidden_units=[4*64],\n",
    "#                 num_trans_blocks=4)\n",
    "\n",
    "# We try a much much smaller one.\n",
    "# This configuration will get a minimal validation loss about 1.70.\n",
    "cfg = GPTConfig(tokenizer.vocab_size, seq_len,\n",
    "                embed_dim=64,\n",
    "                model_dim=64,\n",
    "                num_heads=4,\n",
    "                ffd_hidden_units=[4*64],\n",
    "                num_trans_blocks=2)\n",
    "\n",
    "model = VanillaGPT(cfg)\n",
    "model.build([None, seq_len])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "129f1cce-c536-452d-8cfe-3b2303de497f",
   "metadata": {
    "id": "129f1cce-c536-452d-8cfe-3b2303de497f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-17 14:09:18.816281: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 513944064 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15685/15685 [==============================] - 642s 41ms/step - loss: 1.9147 - val_loss: 1.8757\n",
      "Epoch 2/100\n",
      "15685/15685 [==============================] - 705s 45ms/step - loss: 1.6864 - val_loss: 1.8203\n",
      "Epoch 3/100\n",
      "15685/15685 [==============================] - 666s 42ms/step - loss: 1.6148 - val_loss: 1.7769\n",
      "Epoch 4/100\n",
      "15685/15685 [==============================] - 641s 41ms/step - loss: 1.5720 - val_loss: 1.7427\n",
      "Epoch 5/100\n",
      "15685/15685 [==============================] - 640s 41ms/step - loss: 1.5444 - val_loss: 1.7216\n",
      "Epoch 6/100\n",
      "15685/15685 [==============================] - 668s 43ms/step - loss: 1.5263 - val_loss: 1.7171\n",
      "Epoch 7/100\n",
      "15685/15685 [==============================] - 680s 43ms/step - loss: 1.5122 - val_loss: 1.7159\n",
      "Epoch 8/100\n",
      "15685/15685 [==============================] - 675s 43ms/step - loss: 1.5013 - val_loss: 1.7110\n",
      "Epoch 9/100\n",
      "15685/15685 [==============================] - 678s 43ms/step - loss: 1.4918 - val_loss: 1.7031\n",
      "Epoch 10/100\n",
      " 4499/15685 [=======>......................] - ETA: 7:50 - loss: 1.4808"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m      2\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mAdamW(),\n\u001b[1;32m      3\u001b[0m     loss\u001b[38;5;241m=\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      4\u001b[0m )\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# The epochs argument shall be as large as possible. And we control the\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# true epochs by early-stopping.\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nix/store/4iz1prnx8i6i06l941hpjwgs0a45c87s-python3.11-keras-2.14.0/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/nix/store/4iz1prnx8i6i06l941hpjwgs0a45c87s-python3.11-keras-2.14.0/lib/python3.11/site-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/nix/store/xpfh5nb3dhspkm67a5b8ag17ylrw1fps-python3.11-tensorflow-2.13.0/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/nix/store/xpfh5nb3dhspkm67a5b8ag17ylrw1fps-python3.11-tensorflow-2.13.0/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/nix/store/xpfh5nb3dhspkm67a5b8ag17ylrw1fps-python3.11-tensorflow-2.13.0/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/nix/store/xpfh5nb3dhspkm67a5b8ag17ylrw1fps-python3.11-tensorflow-2.13.0/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nix/store/xpfh5nb3dhspkm67a5b8ag17ylrw1fps-python3.11-tensorflow-2.13.0/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/nix/store/xpfh5nb3dhspkm67a5b8ag17ylrw1fps-python3.11-tensorflow-2.13.0/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m/nix/store/xpfh5nb3dhspkm67a5b8ag17ylrw1fps-python3.11-tensorflow-2.13.0/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m/nix/store/xpfh5nb3dhspkm67a5b8ag17ylrw1fps-python3.11-tensorflow-2.13.0/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=AdamW(),\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    ")\n",
    "model.fit(\n",
    "    x=contexts,\n",
    "    y=targets,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    # The epochs argument shall be as large as possible. And we control the\n",
    "    # true epochs by early-stopping.\n",
    "    epochs=100,\n",
    "    callbacks=[EarlyStopping()]\n",
    ")\n",
    "# For our smaller configuration, the training will overfit after epoch 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "gscxB8dwHaFv",
   "metadata": {
    "id": "gscxB8dwHaFv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l that I have and the state to the prove,\n",
      "Who in the comes and am the mine that her stay\n",
      "The father and the sear to the to the fares.\n",
      "\n",
      "LUCIO:\n",
      "What sweet the breath!\n",
      "\n",
      "LUCIO:\n",
      "Shall be a man stand mistomber which a crown,\n",
      "The cannot men show the father we honour.\n",
      "\n",
      "MENENIUS:\n",
      "If you had with the gracious my good me.\n",
      "\n",
      "DUKE OF AUMERLE:\n",
      "I have be in that the man and in my soul\n",
      "And satch of word the made and many to the shall be\n",
      "hence the man a crown the grieven a strive\n",
      "The and the man the crown the be brother charge.\n",
      "\n",
      "JULIET:\n",
      "O, that is that, I will the touch he service.\n",
      "Not much the burn the crown her brother warriaded,\n",
      "And more a bring than crowned of the triumph.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Madam.\n",
      "\n",
      "First Citizen:\n",
      "I will not the be in the from to the more.\n",
      "\n",
      "First Citizens to the bloody the other stem,\n",
      "Which the contrands to the marry man to me,\n",
      "And these of think of the men your the fear,\n",
      "That the compared a growned that brother for them\n",
      "And the bed the crown for the duke thy brother had\n",
      "the fair the\n"
     ]
    }
   ],
   "source": [
    "generated = model.generate(contexts[0, :], 1000, 0.5)\n",
    "print(tokenizer.decode(generated.numpy())[seq_len:])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
