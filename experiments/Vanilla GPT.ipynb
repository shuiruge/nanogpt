{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eRCpWuVeBogx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eRCpWuVeBogx",
    "outputId": "72eab9ac-9e4e-4448-d3f8-aab258e606f6"
   },
   "outputs": [],
   "source": [
    "# # On Google colab:\n",
    "\n",
    "# # Data\n",
    "# !mkdir data\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "# !mv input.txt data/shakespeare.txt\n",
    "\n",
    "# # Module\n",
    "# !pip install git+https://github.com/shuiruge/nanogpt.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25758ae2-c983-4c56-a1e8-8bdb4858f44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locally:\n",
    "import sys\n",
    "sys.path.append('../nanogpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "250aebf8-3872-4ac5-ae23-a6beed98703e",
   "metadata": {
    "id": "250aebf8-3872-4ac5-ae23-a6beed98703e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-16 20:50:50.418813: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Layer, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import AdamW\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.callbacks import EarlyStopping\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "# from nanogpt.utils import (\n",
    "from utils import (\n",
    "    CharacterTokenizer, LanguageModelDataGenerator, TokPosEmbedding, FeedForward,\n",
    "    luong_attention, MultiHeadWrapper,\n",
    ")\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7f6f09f-948d-4430-90a2-ac20b2310ec5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7f6f09f-948d-4430-90a2-ac20b2310ec5",
    "outputId": "6df1579a-bb00-46ae-8aaa-d9ea5c207c59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "with open('data/shakespeare.txt', 'r') as f:\n",
    "    corpus = ''.join(f.readlines())\n",
    "print(corpus[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "563aecf0-7f80-4e8c-a130-57a8bcff1df0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "563aecf0-7f80-4e8c-a130-57a8bcff1df0",
    "outputId": "8fa93e9e-a4a9-45ce-d17b-e38018bf90e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = CharacterTokenizer(corpus)\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "252be36e-effd-4d54-93dc-bf9bf50aee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 20\n",
    "data = LanguageModelDataGenerator(tokenizer.encode(corpus))\n",
    "\n",
    "contexts = []\n",
    "targets = []\n",
    "while True:\n",
    "    try:\n",
    "        context, target = data(seq_len, False)\n",
    "        contexts.append(context)\n",
    "        targets.append(target)\n",
    "    except StopIteration:\n",
    "        break\n",
    "contexts = np.stack(contexts).astype('int64')\n",
    "targets = np.asarray(targets, 'int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c494a087-80a2-46de-8f16-79f1ae6e7879",
   "metadata": {
    "id": "c494a087-80a2-46de-8f16-79f1ae6e7879"
   },
   "outputs": [],
   "source": [
    "class VanillaSelfAttention(Layer):\n",
    "    \"\"\"A vanilla version of self-attention.\n",
    "    \n",
    "    It is called vanilla because it has no trainable variables at all! The\n",
    "    function of this kind of self-attention is just communicating between\n",
    "    each token (or each node if there is an image of community in your mind).\n",
    "    So, it has no resposibility for computation, which is completely left to\n",
    "    feed-forward layers.\n",
    "\n",
    "    In this communication viewpoint, the multi-head means the multi-channel\n",
    "    of communication. Each channel propagates one kind of information. And\n",
    "    different kinds of information are sent to different target nodes.\n",
    "\n",
    "    Args:\n",
    "        num_heads: int\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.multihead = MultiHeadWrapper(self.num_heads)\n",
    "\n",
    "    def call(self, x, return_weights=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: tf.Tensor\n",
    "                Shape [..., seq_len, dim]\n",
    "            return_weights: bool\n",
    "                If return the attention weights. Defaults to False.\n",
    "\n",
    "        Returns: tf.Tensor\n",
    "            The same shape and dtype as the x.\n",
    "        \"\"\"\n",
    "        query = self.multihead.split_heads(x)\n",
    "        key = self.multihead.split_heads(x)\n",
    "        value = self.multihead.split_heads(x)\n",
    "\n",
    "        output, attention_weights = luong_attention(query, key, value)\n",
    "        output = self.multihead.concat_heads(output)\n",
    "        return (output, attention_weights) if return_weights else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e26b40f9-380f-4309-9e3a-37c8458ad0e8",
   "metadata": {
    "id": "e26b40f9-380f-4309-9e3a-37c8458ad0e8"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int\n",
    "    seq_len: int\n",
    "    embed_dim: int\n",
    "    model_dim: int\n",
    "    num_heads: int\n",
    "    ffd_hidden_units: List[int]\n",
    "    num_trans_blocks: int\n",
    "\n",
    "\n",
    "class VanillaGPT(Model):\n",
    "    \"\"\"Build a vanilla GPT with vanilla self-attention.\"\"\"\n",
    "\n",
    "    def __init__(self, cfg: GPTConfig, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.embedding_layer = TokPosEmbedding(\n",
    "            cfg.vocab_size, cfg.seq_len, cfg.embed_dim)\n",
    "\n",
    "        # The so-called transformer-blocks.\n",
    "        self.trans_blocks = []\n",
    "        for _ in range(cfg.num_trans_blocks):\n",
    "            # As we have discussed in the docstring of VanillaSelfAttention,\n",
    "            # the task of computation is left to feed-forward layers. So,\n",
    "            # for communication:\n",
    "            self.trans_blocks.append(\n",
    "                VanillaSelfAttention(cfg.num_heads)\n",
    "            )\n",
    "            # and for computation:\n",
    "            self.trans_blocks.append(\n",
    "                FeedForward(cfg.ffd_hidden_units, cfg.model_dim)\n",
    "            )\n",
    "\n",
    "        self.output_layer = Dense(cfg.vocab_size)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding_layer(x)\n",
    "        for layer in self.trans_blocks:\n",
    "            x = layer(x)\n",
    "        # We only use the last sequence element for output.\n",
    "        # It is such a waste!\n",
    "        # And why it is the last one? Why not the others? Maybe some of the\n",
    "        # others out-performs the last.\n",
    "        x = x[:, -1, :]\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def generate(self, init_token_ids, num_new_tokens, T):\n",
    "        \"\"\"Generates new tokens from the initial.\n",
    "\n",
    "        The \"temperature\" T controls the randomness, as in the Boltzmann\n",
    "        distributions.\n",
    "\n",
    "        Args:\n",
    "            init_token_ids: List[int]\n",
    "            num_new_tokens: int\n",
    "            T: float\n",
    "\n",
    "        Returns: List[int]\n",
    "            It also includes the initial token-IDs. So, the length is the\n",
    "            `len(initial_token_ids) + \n",
    "        \"\"\"\n",
    "        init_token_ids = tf.convert_to_tensor(init_token_ids)\n",
    "\n",
    "        # Add batch_size for matching the input shape of `self.call`.\n",
    "        # [1, len(init_token_ids)]\n",
    "        token_ids = tf.expand_dims(init_token_ids, axis=0)\n",
    "\n",
    "        for _ in range(num_new_tokens):\n",
    "            # [1, vocab_size]\n",
    "            logits = self(token_ids[:, -self.cfg.seq_len:])\n",
    "            # [1, 1]\n",
    "            next_token_id = tf.random.categorical(logits/T, 1)\n",
    "            token_ids = tf.concat([token_ids, next_token_id], axis=1)\n",
    "\n",
    "        # Drop the batch_size\n",
    "        token_ids = tf.squeeze(token_ids, axis=0)\n",
    "        return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c963ba25-d689-48e5-a736-6ef1a38446ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c963ba25-d689-48e5-a736-6ef1a38446ba",
    "outputId": "f13adb6b-4e8e-4ea5-b8bb-b3b2aee64727"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vanilla_gpt\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " tok_pos_embedding (TokPosE  multiple                  5440      \n",
      " mbedding)                                                       \n",
      "                                                                 \n",
      " vanilla_self_attention (Va  multiple                  0         \n",
      " nillaSelfAttention)                                             \n",
      "                                                                 \n",
      " feed_forward (FeedForward)  multiple                  33216     \n",
      "                                                                 \n",
      " vanilla_self_attention_1 (  multiple                  0         \n",
      " VanillaSelfAttention)                                           \n",
      "                                                                 \n",
      " feed_forward_1 (FeedForwar  multiple                  33216     \n",
      " d)                                                              \n",
      "                                                                 \n",
      " dense_4 (Dense)             multiple                  4225      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 76097 (297.25 KB)\n",
      "Trainable params: 76097 (297.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Andrej Karpathy's configuration.\n",
    "# See: https://www.youtube.com/watch?v=kCc8FmEb1nY&t=6068s, at 1:40:30.\n",
    "# This configuration will get a minimal validation loss about 1.48.\n",
    "# And this configuration is not for our vanilla GPT, but Andrej's nano GPT,\n",
    "# which is much more complicated than ours.\n",
    "# cfg = GPTConfig(tokenizer.vocab_size, seq_len,\n",
    "#                 embed_dim=384,\n",
    "#                 model_dim=384,\n",
    "#                 num_heads=6,\n",
    "#                 ffd_hidden_units=[4*384],\n",
    "#                 num_trans_blocks=6)\n",
    "\n",
    "# We try a much much smaller one.\n",
    "# This configuration will get a minimal validation loss about 1.77.\n",
    "cfg = GPTConfig(tokenizer.vocab_size, seq_len,\n",
    "                embed_dim=64,\n",
    "                model_dim=64,\n",
    "                num_heads=4,\n",
    "                ffd_hidden_units=[256],\n",
    "                num_trans_blocks=2)\n",
    "\n",
    "model = VanillaGPT(cfg)\n",
    "model.build([None, seq_len])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "129f1cce-c536-452d-8cfe-3b2303de497f",
   "metadata": {
    "id": "129f1cce-c536-452d-8cfe-3b2303de497f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15685/15685 [==============================] - 142s 9ms/step - loss: 2.9399 - val_loss: 2.6986\n",
      "Epoch 2/100\n",
      "15685/15685 [==============================] - 139s 9ms/step - loss: 2.0457 - val_loss: 1.9839\n",
      "Epoch 3/100\n",
      "15685/15685 [==============================] - 142s 9ms/step - loss: 1.7705 - val_loss: 1.8922\n",
      "Epoch 4/100\n",
      "15685/15685 [==============================] - 142s 9ms/step - loss: 1.6799 - val_loss: 1.8313\n",
      "Epoch 5/100\n",
      "15685/15685 [==============================] - 153s 10ms/step - loss: 1.6315 - val_loss: 1.7912\n",
      "Epoch 6/100\n",
      "15685/15685 [==============================] - 172s 11ms/step - loss: 1.5993 - val_loss: 1.7874\n",
      "Epoch 7/100\n",
      "15685/15685 [==============================] - 161s 10ms/step - loss: 1.5768 - val_loss: 1.7691\n",
      "Epoch 8/100\n",
      "15685/15685 [==============================] - 154s 10ms/step - loss: 1.5592 - val_loss: 1.7564\n",
      "Epoch 9/100\n",
      "15685/15685 [==============================] - 151s 10ms/step - loss: 1.5454 - val_loss: 1.7274\n",
      "Epoch 10/100\n",
      "15685/15685 [==============================] - 156s 10ms/step - loss: 1.5340 - val_loss: 1.7308\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fd41256fb10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=AdamW(),\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    ")\n",
    "model.fit(\n",
    "    x=contexts,\n",
    "    y=targets,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    # The epochs argument shall be as large as possible. And we control the\n",
    "    # true epochs by early-stopping.\n",
    "    epochs=100,\n",
    "    callbacks=[EarlyStopping()]\n",
    ")\n",
    "# For our smaller configuration, the training will overfit after epoch 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "gscxB8dwHaFv",
   "metadata": {
    "id": "gscxB8dwHaFv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e the be present.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "No, the call me the say to our grave to the own they father be so and the spill as the in dear of by the promise the send him for the with the have means to be consent when and to for my come other to be the dead.\n",
      "\n",
      "First Servingman:\n",
      "Thou will not provost to the words and would Call the me;\n",
      "And the not shall the will not for the for the deed.\n",
      "\n",
      "STANLEY:\n",
      "Hark your poor of the droolw.\n",
      "\n",
      "CLARENCE:\n",
      "The shall one send me say and we did consent not more to the provost,\n"
     ]
    }
   ],
   "source": [
    "generated = model.generate(contexts[0, :], 500, 0.5)\n",
    "print(tokenizer.decode(generated.numpy())[seq_len:])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
