{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eRCpWuVeBogx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eRCpWuVeBogx",
    "outputId": "72eab9ac-9e4e-4448-d3f8-aab258e606f6"
   },
   "outputs": [],
   "source": [
    "# # On Google colab:\n",
    "\n",
    "# # Data\n",
    "# !mkdir data\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "# !mv input.txt data/shakespeare.txt\n",
    "\n",
    "# # Module\n",
    "# !pip install git+https://github.com/shuiruge/nanogpt.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25758ae2-c983-4c56-a1e8-8bdb4858f44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locally:\n",
    "import sys\n",
    "sys.path.append('../nanogpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "250aebf8-3872-4ac5-ae23-a6beed98703e",
   "metadata": {
    "id": "250aebf8-3872-4ac5-ae23-a6beed98703e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 23:13:55.249361: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Layer, Dense, LayerNormalization, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import AdamW\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.callbacks import EarlyStopping\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "# from nanogpt.utils import (\n",
    "from utils import (\n",
    "    CharacterTokenizer, TokPosEmbedding, FeedForward, MultiHeadSelfAttention,\n",
    "    ResNetWrapper, MaskedLanguageModelDataGenerator,\n",
    ")\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7f6f09f-948d-4430-90a2-ac20b2310ec5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7f6f09f-948d-4430-90a2-ac20b2310ec5",
    "outputId": "6df1579a-bb00-46ae-8aaa-d9ea5c207c59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "with open('data/shakespeare.txt', 'r') as f:\n",
    "    corpus = ''.join(f.readlines())\n",
    "print(corpus[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "563aecf0-7f80-4e8c-a130-57a8bcff1df0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "563aecf0-7f80-4e8c-a130-57a8bcff1df0",
    "outputId": "8fa93e9e-a4a9-45ce-d17b-e38018bf90e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66, 65)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = CharacterTokenizer(corpus, placeholders=['<MASK>'])\n",
    "mask_token_id = tokenizer.get_id('<MASK>')\n",
    "tokenizer.vocab_size, mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "252be36e-effd-4d54-93dc-bf9bf50aee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 64\n",
    "num_mask = 1\n",
    "get_mask_positions = lambda context: [len(context)-i-1 for i in range(num_mask)]\n",
    "data = MaskedLanguageModelDataGenerator(\n",
    "    tokenizer.encode(corpus), mask_token_id, get_mask_positions)\n",
    "\n",
    "contexts = []\n",
    "targets = []\n",
    "masks = []\n",
    "while True:\n",
    "    try:\n",
    "        context, target, mask = data(seq_len, False)\n",
    "        contexts.append(context)\n",
    "        targets.append(target)\n",
    "        masks.append(mask)\n",
    "    except StopIteration:\n",
    "        break\n",
    "contexts = np.stack(contexts).astype('int64')\n",
    "targets = np.asarray(targets, 'int64')\n",
    "masks = np.asarray(masks, 'int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d78add76-280f-4f62-90af-3d708ac72b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43,\n",
       "        44, 53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39,\n",
       "        52, 63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1,\n",
       "        51, 43,  1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 65]),\n",
       " array([50]),\n",
       " array([63]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts[0], targets[0], masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e26b40f9-380f-4309-9e3a-37c8458ad0e8",
   "metadata": {
    "id": "e26b40f9-380f-4309-9e3a-37c8458ad0e8"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BERTConfig:\n",
    "    vocab_size: int\n",
    "    seq_len: int\n",
    "    embed_dim: int\n",
    "    model_dim: int\n",
    "    num_heads: int\n",
    "    ffd_hidden_units: List[int]\n",
    "    num_trans_blocks: int\n",
    "    mask_token_id: int\n",
    "\n",
    "\n",
    "class BERT(Model):\n",
    "\n",
    "    def __init__(self, cfg: BERTConfig, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.embedding_layer = TokPosEmbedding(\n",
    "            cfg.vocab_size, cfg.seq_len, cfg.embed_dim)\n",
    "\n",
    "        self.trans_blocks = []\n",
    "        for _ in range(cfg.num_trans_blocks):\n",
    "            self.trans_blocks.append(\n",
    "                ResNetWrapper(MultiHeadSelfAttention(cfg.model_dim, cfg.num_heads))\n",
    "            )\n",
    "            self.trans_blocks.append(\n",
    "                ResNetWrapper(FeedForward(cfg.ffd_hidden_units, cfg.model_dim))\n",
    "            )\n",
    "\n",
    "        self.output_layer = Dense(cfg.vocab_size)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, mask_positions = inputs\n",
    "        x = self.embedding_layer(x)\n",
    "        for layer in self.trans_blocks:\n",
    "            x = layer(x)\n",
    "        # x has shape [batch_size, seq_len, model_dim].\n",
    "        # See: https://www.tensorflow.org/api_docs/python/tf/gather#batching\n",
    "        x = tf.gather(x, indices=mask_positions, axis=1, batch_dims=1)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def generate(self, init_token_ids, num_new_tokens, max_iter, T):\n",
    "        token_ids = list(init_token_ids)\n",
    "        num_ref = len(init_token_ids) - num_new_tokens\n",
    "        for _ in range(max_iter):\n",
    "            input = (\n",
    "                token_ids[-num_ref:] +\n",
    "                [self.cfg.mask_token_id for _ in range(num_new_tokens)]\n",
    "            )\n",
    "            # Add batch_size for matching the input shape of `self.call`.\n",
    "            input = tf.expand_dims(input, axis=0)\n",
    "            mask = [[num_ref+i for i in range(num_new_tokens)]]\n",
    "            # [1, num_new_tokens, vocab_size]\n",
    "            logits = self((input, mask))\n",
    "            # [num_new_tokens, vocab_size]\n",
    "            logits = logits[0, :, :]\n",
    "            # [num_new_tokens]\n",
    "            next_token_ids = tf.random.categorical(logits/T, 1)\n",
    "            token_ids += list(next_token_ids[:, 0].numpy())\n",
    "        return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c963ba25-d689-48e5-a736-6ef1a38446ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c963ba25-d689-48e5-a736-6ef1a38446ba",
    "outputId": "f13adb6b-4e8e-4ea5-b8bb-b3b2aee64727"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bert\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " tok_pos_embedding (TokPosE  multiple                  8320      \n",
      " mbedding)                                                       \n",
      "                                                                 \n",
      " resnet_wrapper_of_multi_he  multiple                  8320      \n",
      " ad_self_attention (ResNetW                                      \n",
      " rapper)                                                         \n",
      "                                                                 \n",
      " resnet_wrapper_of_feed_for  multiple                  33216     \n",
      " ward (ResNetWrapper)                                            \n",
      "                                                                 \n",
      " resnet_wrapper_of_multi_he  multiple                  8320      \n",
      " ad_self_attention_1 (ResNe                                      \n",
      " tWrapper)                                                       \n",
      "                                                                 \n",
      " resnet_wrapper_of_feed_for  multiple                  33216     \n",
      " ward_1 (ResNetWrapper)                                          \n",
      "                                                                 \n",
      " dense_8 (Dense)             multiple                  4290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 95682 (373.76 KB)\n",
      "Trainable params: 95682 (373.76 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cfg = BERTConfig(\n",
    "    tokenizer.vocab_size, seq_len,\n",
    "    embed_dim=64,\n",
    "    model_dim=64,\n",
    "    num_heads=4,\n",
    "    ffd_hidden_units=[4*64],\n",
    "    num_trans_blocks=2,\n",
    "    mask_token_id=mask_token_id,\n",
    ")\n",
    "\n",
    "model = BERT(cfg)\n",
    "# model.build([[None, seq_len], [None, num_mask]])\n",
    "model((contexts[:10], masks[:10]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "129f1cce-c536-452d-8cfe-3b2303de497f",
   "metadata": {
    "id": "129f1cce-c536-452d-8cfe-3b2303de497f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 23:26:28.836144: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 513944064 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6590/15685 [===========>..................] - ETA: 7:14 - loss: 1.6575"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=AdamW(),\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    ")\n",
    "model.fit(\n",
    "    x=(contexts, masks),\n",
    "    y=targets,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    # The epochs argument shall be as large as possible. And we control the\n",
    "    # true epochs by early-stopping.\n",
    "    epochs=100,\n",
    "    callbacks=[EarlyStopping()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c676267-f111-4ab8-9db4-52f538a2b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(self, init_token_ids, num_new_tokens, max_iter, T):\n",
    "    token_ids = list(init_token_ids)\n",
    "    num_ref = len(init_token_ids) - num_new_tokens\n",
    "    for _ in range(max_iter):\n",
    "        input = (\n",
    "            token_ids[-num_ref:] +\n",
    "            [self.cfg.mask_token_id for _ in range(num_new_tokens)]\n",
    "        )\n",
    "        # Add batch_size for matching the input shape of `self.call`.\n",
    "        input = tf.expand_dims(input, axis=0)\n",
    "        mask = [[num_ref+i for i in range(num_new_tokens)]]\n",
    "        # [1, num_new_tokens, vocab_size]\n",
    "        logits = self((input, mask))\n",
    "        # [num_new_tokens, vocab_size]\n",
    "        logits = logits[0, :, :]\n",
    "        # [num_new_tokens]\n",
    "        next_token_ids = tf.random.categorical(logits/T, 1)\n",
    "        token_ids += list(next_token_ids[:, 0].numpy())\n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "gscxB8dwHaFv",
   "metadata": {
    "id": "gscxB8dwHaFv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "AhuSto\n",
      "dst,cl r ;fst  pissrnni u''Tt r't tu,i dr;ttictta tene.t.i?Sent,ti t'ys et,r:t;ad' ,'s tnts u?'ssi 'b,i  nhi ssse ei t!at\n",
      "ii'nsy.ur' ; rei?'s is ti, tr y.tuttcos e er\n",
      "u, niit',.e ,ff',tr;y aset,teuttet,titr ysasst tu t  emt.at'm'ttsi,,e i,ti'i.ti\n",
      "n,an''?'t;t an nu' 'mtotit,rneiiati,i; ,ei,''a en.u ,tntsTssy pn utos t,sryetis!t,ts\n",
      "e,tmou 'r rtrs te n itsd , y 'I;e  th,iem!ss'ttt si .-ti,ss.i tn.ttaulet's'i ', tt'nei ta,i,us't ',,ie n a,irsttonse!t!\n",
      "!aeeny!ts., 's,otat,t,ai , dn e!tsst Ui ts\n"
     ]
    }
   ],
   "source": [
    "generated = generate(model, contexts[0, :-num_mask], 5, 100, 1)\n",
    "print(tokenizer.decode(generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "086e450b-1ad8-4d43-bd8a-8d9103d89514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(contexts[0, :-num_mask]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
