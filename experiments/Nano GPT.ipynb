{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "eRCpWuVeBogx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRCpWuVeBogx",
        "outputId": "eb544197-2df9-4cd4-db3e-5e53d3ccd53e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-18 15:44:44--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-03-18 15:44:44 (19.9 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n",
            "Collecting git+https://github.com/shuiruge/nanogpt.git\n",
            "  Cloning https://github.com/shuiruge/nanogpt.git to /tmp/pip-req-build-5o3q876g\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/shuiruge/nanogpt.git /tmp/pip-req-build-5o3q876g\n",
            "  Resolved https://github.com/shuiruge/nanogpt.git to commit 23a263958d209b0ec4055aa76cad02c0f20e6539\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: nanogpt\n",
            "  Building wheel for nanogpt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nanogpt: filename=nanogpt-0.0.1-py3-none-any.whl size=5845 sha256=eca2cf16736998a1e33ef72981a37d5c379de63b44de0c0cc2294dbb9a8f7d83\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-o3hll558/wheels/5b/10/e2/56b89782f042b87ab2c6d4c263a5843c0498b8d94e8dfc42c7\n",
            "Successfully built nanogpt\n",
            "Installing collected packages: nanogpt\n",
            "Successfully installed nanogpt-0.0.1\n"
          ]
        }
      ],
      "source": [
        "# On Google colab:\n",
        "\n",
        "# Data\n",
        "!mkdir data\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "!mv input.txt data/shakespeare.txt\n",
        "\n",
        "# Module\n",
        "!pip install git+https://github.com/shuiruge/nanogpt.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "25758ae2-c983-4c56-a1e8-8bdb4858f44a",
      "metadata": {
        "id": "25758ae2-c983-4c56-a1e8-8bdb4858f44a"
      },
      "outputs": [],
      "source": [
        "# # Locally:\n",
        "# import sys\n",
        "# sys.path.append('../nanogpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "250aebf8-3872-4ac5-ae23-a6beed98703e",
      "metadata": {
        "id": "250aebf8-3872-4ac5-ae23-a6beed98703e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.layers import Layer, Dense, LayerNormalization, Dropout\n",
        "from keras.models import Model\n",
        "from keras.optimizers import AdamW\n",
        "from keras.losses import SparseCategoricalCrossentropy\n",
        "from keras.callbacks import EarlyStopping\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "from nanogpt.utils import (\n",
        "# from utils import (\n",
        "    CharacterTokenizer, LanguageModelDataGenerator, TokPosEmbedding, FeedForward,\n",
        "    ResNetWrapper, MultiHeadSelfAttention,\n",
        ")\n",
        "\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d7f6f09f-948d-4430-90a2-ac20b2310ec5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7f6f09f-948d-4430-90a2-ac20b2310ec5",
        "outputId": "cb204dfa-6844-400a-bc0d-fd520008893c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you\n"
          ]
        }
      ],
      "source": [
        "with open('data/shakespeare.txt', 'r') as f:\n",
        "    corpus = ''.join(f.readlines())\n",
        "print(corpus[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "563aecf0-7f80-4e8c-a130-57a8bcff1df0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "563aecf0-7f80-4e8c-a130-57a8bcff1df0",
        "outputId": "6b02f327-e9e2-4036-cfdc-9156819edfe3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "tokenizer = CharacterTokenizer(corpus)\n",
        "tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "252be36e-effd-4d54-93dc-bf9bf50aee6a",
      "metadata": {
        "id": "252be36e-effd-4d54-93dc-bf9bf50aee6a"
      },
      "outputs": [],
      "source": [
        "seq_len = 64\n",
        "data = LanguageModelDataGenerator(tokenizer.encode(corpus))\n",
        "\n",
        "contexts = []\n",
        "targets = []\n",
        "context = None\n",
        "while True:\n",
        "    try:\n",
        "        next_context, _ = data(seq_len, False)\n",
        "        if context is None:\n",
        "            context = next_context\n",
        "            continue\n",
        "        contexts.append(context)\n",
        "        targets.append(next_context)\n",
        "        context = next_context\n",
        "    except StopIteration:\n",
        "        break\n",
        "contexts = np.stack(contexts).astype('int64')\n",
        "targets = np.stack(targets).astype('int64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "24099cff-8787-407e-a831-bf74d35a0c1e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24099cff-8787-407e-a831-bf74d35a0c1e",
        "outputId": "0c9e237c-66b3-428b-dbd7-816792b7d4c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14,\n",
              "         43, 44, 53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,\n",
              "          1, 39, 52, 63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43,\n",
              "         39, 56,  1, 51, 43,  1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50],\n",
              "        [47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43,\n",
              "         44, 53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1,\n",
              "         39, 52, 63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39,\n",
              "         56,  1, 51, 43,  1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50],\n",
              "        [56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
              "         53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39,\n",
              "         52, 63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,\n",
              "          1, 51, 43,  1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10]]),\n",
              " array([[47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43,\n",
              "         44, 53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1,\n",
              "         39, 52, 63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39,\n",
              "         56,  1, 51, 43,  1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50],\n",
              "        [56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
              "         53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39,\n",
              "         52, 63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,\n",
              "          1, 51, 43,  1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10],\n",
              "        [57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44, 53,\n",
              "         56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52,\n",
              "         63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1,\n",
              "         51, 43,  1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0]]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "contexts[:3], targets[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c494a087-80a2-46de-8f16-79f1ae6e7879",
      "metadata": {
        "id": "c494a087-80a2-46de-8f16-79f1ae6e7879"
      },
      "outputs": [],
      "source": [
        "# class CausalSelfAttention(Layer):\n",
        "#     \"\"\"A standard version of self-attention for GPT.\n",
        "\n",
        "#     Args:\n",
        "#         num_heads: int\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, model_dim, num_heads):\n",
        "#         super().__init__()\n",
        "#         self.model_dim = model_dim\n",
        "#         self.num_heads = num_heads\n",
        "\n",
        "#         self.multihead = MultiHeadWrapper(self.num_heads)\n",
        "#         self.get_query = Dense(self.model_dim)\n",
        "#         self.get_key = Dense(self.model_dim)\n",
        "#         self.get_value = Dense(self.model_dim)\n",
        "\n",
        "#     def call(self, x, return_weights=False):\n",
        "#         \"\"\"\n",
        "#         Args:\n",
        "#             x: tf.Tensor\n",
        "#                 Shape [..., seq_len, dim]\n",
        "#             return_weights: bool\n",
        "#                 If return the attention weights. Defaults to False.\n",
        "\n",
        "#         Returns: tf.Tensor or (tf.Tensor, tf.Tensor)\n",
        "#             If return_weights is false, then return the output only, which has\n",
        "#             the same shape and dtype as the x. Otherwise, return the output as\n",
        "#             well as the attention weights, which has shape\n",
        "#             [..., num_heads, seq_len, seq_len].\n",
        "#         \"\"\"\n",
        "#         query = self.get_query(x)\n",
        "#         key = self.get_key(x)\n",
        "#         value = self.get_value(x)\n",
        "\n",
        "#         # Split the last dimension into multiple heads.\n",
        "#         query = self.multihead.split_heads(query)\n",
        "#         key = self.multihead.split_heads(key)\n",
        "#         value = self.multihead.split_heads(value)\n",
        "\n",
        "#         # Mask the self-communication.\n",
        "#         seq_len = tf.shape(x)[-2]\n",
        "#         # Like\n",
        "#         # [[0., 1., 1., 1.],\n",
        "#         #  [0., 0., 1., 1.],\n",
        "#         #  [0., 0., 0., 1.],\n",
        "#         #  [0., 0., 0., 0.]]\n",
        "#         mask = 1 - tf.linalg.band_part(tf.ones([seq_len, seq_len]), -1, 0)\n",
        "\n",
        "#         # The communication is implemented by a Luong-style attention.\n",
        "#         output, attention_weights = luong_attention(query, key, value, mask)\n",
        "\n",
        "#         # Concatenate the heads together.\n",
        "#         output = self.multihead.concat_heads(output)\n",
        "\n",
        "#         # The ResNet trick\n",
        "#         return (output, attention_weights) if return_weights else output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9d8b7bcc-ae01-4b27-afd9-cf16d5664b38",
      "metadata": {
        "id": "9d8b7bcc-ae01-4b27-afd9-cf16d5664b38"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(Layer):\n",
        "    \"\"\"A standard version of self-attention for GPT.\n",
        "\n",
        "    Args:\n",
        "        model_dim: int\n",
        "        num_heads: int\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.model_dim = model_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.self_attention = MultiHeadSelfAttention(model_dim, num_heads)\n",
        "\n",
        "    def call(self, x, return_weights=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: tf.Tensor\n",
        "                Shape [..., seq_len, dim]\n",
        "            return_weights: bool\n",
        "                If return the attention weights. Defaults to False.\n",
        "\n",
        "        Returns: tf.Tensor or (tf.Tensor, tf.Tensor)\n",
        "            If return_weights is false, then return the output only, which has\n",
        "            the same shape and dtype as the x. Otherwise, return the output as\n",
        "            well as the attention weights, which has shape\n",
        "            [..., num_heads, seq_len, seq_len].\n",
        "        \"\"\"\n",
        "        seq_len = tf.shape(x)[-2]\n",
        "        # Like\n",
        "        # [[0., 1., 1., 1.],\n",
        "        #  [0., 0., 1., 1.],\n",
        "        #  [0., 0., 0., 1.],\n",
        "        #  [0., 0., 0., 0.]]\n",
        "        mask = 1 - tf.linalg.band_part(tf.ones([seq_len, seq_len]), -1, 0)\n",
        "        return self.self_attention(x, mask, return_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e26b40f9-380f-4309-9e3a-37c8458ad0e8",
      "metadata": {
        "id": "e26b40f9-380f-4309-9e3a-37c8458ad0e8"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    vocab_size: int\n",
        "    seq_len: int\n",
        "    embed_dim: int\n",
        "    model_dim: int\n",
        "    num_heads: int\n",
        "    ffd_hidden_units: List[int]\n",
        "    num_trans_blocks: int\n",
        "\n",
        "\n",
        "class NanoGPT(Model):\n",
        "\n",
        "    def __init__(self, cfg: GPTConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "        self.embedding_layer = TokPosEmbedding(\n",
        "            cfg.vocab_size, cfg.seq_len, cfg.embed_dim)\n",
        "\n",
        "        # The so-called transformer-blocks.\n",
        "        self.trans_blocks = []\n",
        "        for _ in range(cfg.num_trans_blocks):\n",
        "            self.trans_blocks.append(\n",
        "                ResNetWrapper(CausalSelfAttention(cfg.model_dim, cfg.num_heads))\n",
        "            )\n",
        "            self.trans_blocks.append(\n",
        "                ResNetWrapper(FeedForward(cfg.ffd_hidden_units, cfg.model_dim))\n",
        "            )\n",
        "\n",
        "        self.output_layer = Dense(cfg.vocab_size)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embedding_layer(x)\n",
        "        for layer in self.trans_blocks:\n",
        "            x = layer(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "    def generate(self, init_token_ids, num_new_tokens, T):\n",
        "        \"\"\"Generates new tokens from the initial.\n",
        "\n",
        "        The \"temperature\" T controls the randomness, as in the Boltzmann\n",
        "        distributions.\n",
        "\n",
        "        Args:\n",
        "            init_token_ids: List[int]\n",
        "            num_new_tokens: int\n",
        "            T: float\n",
        "\n",
        "        Returns: List[int]\n",
        "            It also includes the initial token-IDs. So, the length is the\n",
        "            `len(initial_token_ids) +\n",
        "        \"\"\"\n",
        "        init_token_ids = tf.convert_to_tensor(init_token_ids)\n",
        "\n",
        "        # Add batch_size for matching the input shape of `self.call`.\n",
        "        # [1, len(init_token_ids)]\n",
        "        token_ids = tf.expand_dims(init_token_ids, axis=0)\n",
        "\n",
        "        for _ in range(num_new_tokens):\n",
        "            # [1, seq_len, vocab_size]\n",
        "            logits = self(token_ids[:, -self.cfg.seq_len:])\n",
        "            # We only use the last sequence element for output.\n",
        "            # [1, vocab_size]\n",
        "            logits = logits[:, -1, :]\n",
        "            # [1, 1]\n",
        "            next_token_id = tf.random.categorical(logits/T, 1)\n",
        "            token_ids = tf.concat([token_ids, next_token_id], axis=1)\n",
        "\n",
        "        # Drop the batch_size\n",
        "        token_ids = tf.squeeze(token_ids, axis=0)\n",
        "        return token_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c963ba25-d689-48e5-a736-6ef1a38446ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c963ba25-d689-48e5-a736-6ef1a38446ba",
        "outputId": "3d9139b6-e23f-489d-c504-251a009bcc2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"nano_gpt\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " tok_pos_embedding (TokPosE  multiple                  8256      \n",
            " mbedding)                                                       \n",
            "                                                                 \n",
            " resnet_wrapper_of_causal_s  multiple                  8320      \n",
            " elf_attention (ResNetWrapp                                      \n",
            " er)                                                             \n",
            "                                                                 \n",
            " resnet_wrapper_of_feed_for  multiple                  33216     \n",
            " ward (ResNetWrapper)                                            \n",
            "                                                                 \n",
            " resnet_wrapper_of_causal_s  multiple                  8320      \n",
            " elf_attention_1 (ResNetWra                                      \n",
            " pper)                                                           \n",
            "                                                                 \n",
            " resnet_wrapper_of_feed_for  multiple                  33216     \n",
            " ward_1 (ResNetWrapper)                                          \n",
            "                                                                 \n",
            " dense_8 (Dense)             multiple                  4225      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 95553 (373.25 KB)\n",
            "Trainable params: 95553 (373.25 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Andrej Karpathy's configuration.\n",
        "# See: https://www.youtube.com/watch?v=kCc8FmEb1nY&t=6068s, at 1:40:30.\n",
        "# This configuration will get a minimal validation loss about 1.48.\n",
        "# And this configuration is not for our vanilla GPT, but Andrej's nano GPT,\n",
        "# which is much more complicated than ours.\n",
        "# cfg = GPTConfig(tokenizer.vocab_size, seq_len,\n",
        "#                 embed_dim=384,\n",
        "#                 model_dim=384,\n",
        "#                 num_heads=6,\n",
        "#                 ffd_hidden_units=[4*384],\n",
        "#                 num_trans_blocks=6)\n",
        "\n",
        "# We try a much much smaller one.\n",
        "cfg = GPTConfig(tokenizer.vocab_size, seq_len,\n",
        "                embed_dim=64,\n",
        "                model_dim=64,\n",
        "                num_heads=4,\n",
        "                ffd_hidden_units=[4*64],\n",
        "                num_trans_blocks=2)\n",
        "\n",
        "model = NanoGPT(cfg)\n",
        "model.build([None, seq_len])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6ed79d10-1ad1-4350-909d-44e2df6a28cc",
      "metadata": {
        "id": "6ed79d10-1ad1-4350-909d-44e2df6a28cc"
      },
      "outputs": [],
      "source": [
        "# model(contexts[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "129f1cce-c536-452d-8cfe-3b2303de497f",
      "metadata": {
        "id": "129f1cce-c536-452d-8cfe-3b2303de497f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15880ce6-d49b-40cb-ca87-b690af571066"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "15685/15685 [==============================] - 228s 14ms/step - loss: 1.5771 - val_loss: 1.7034\n",
            "Epoch 2/100\n",
            "15685/15685 [==============================] - 198s 13ms/step - loss: 1.4334 - val_loss: 1.6972\n",
            "Epoch 3/100\n",
            "15685/15685 [==============================] - 201s 13ms/step - loss: 1.4036 - val_loss: 1.6803\n",
            "Epoch 4/100\n",
            "15685/15685 [==============================] - 197s 13ms/step - loss: 1.3874 - val_loss: 1.6926\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d8b36f142b0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "model.compile(\n",
        "    optimizer=AdamW(),\n",
        "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
        ")\n",
        "model.fit(\n",
        "    x=contexts,\n",
        "    y=targets,\n",
        "    batch_size=64,\n",
        "    validation_split=0.1,\n",
        "    # The epochs argument shall be as large as possible. And we control the\n",
        "    # true epochs by early-stopping.\n",
        "    epochs=100,\n",
        "    callbacks=[EarlyStopping()]\n",
        ")\n",
        "# For our smaller configuration, the training will overfit after epoch 9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "gscxB8dwHaFv",
      "metadata": {
        "id": "gscxB8dwHaFv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e29b189-46ec-4229-ea41-cfcbcc4a81dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "What thou art thou hast and my father should be repose and the prince,\n",
            "That thou hast have the people of the sent and with the heads\n",
            "That the fire some of the king to the charged of the\n",
            "To fair the did of the that should be so much a traitor\n",
            "And the duke of the son of the best of the send of the people of the cold\n",
            "That the langer of the sun, and the rest,\n",
            "And that with the fire the commanded of the world and with\n",
            "That the commanded of the respect and the world\n",
            "The traitor house and the courteous and to the soul,\n",
            "The people of the people of the rest of the hand\n",
            "That the was the world to the prince,\n",
            "And the seat the heads of his son and the king to the made of his death\n",
            "The people of the soul, and the way the best\n",
            "The state of the king of the wind of the world\n",
            "That the manner of the world bed, and the seater\n",
            "And by the people the soul, and will stay there the present of the world.\n",
            "\n",
            "KING RICHARD III:\n",
            "What says you the dear will see the earth\n",
            "That the death the poison of the senator of \n"
          ]
        }
      ],
      "source": [
        "generated = model.generate(contexts[0, :], 1000, 0.25)\n",
        "print(tokenizer.decode(generated.numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated = model.generate(contexts[0, :], 1000, 0.5)\n",
        "print(tokenizer.decode(generated.numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ax0kPn3TWXh",
        "outputId": "8d392069-dbc3-455c-c662-17ee1ff61a5a"
      },
      "id": "4Ax0kPn3TWXh",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "The poster's if that less, and I will be parted against the queen of the\n",
            "the courteous the senated stays to his so fair\n",
            "Your purpose and look of the world false and blood;\n",
            "So many crown to her by the place and courage thee,\n",
            "Though the senate and of these my tongue and their to heart.\n",
            "\n",
            "KING RICHARD III:\n",
            "What is the pardon'd and love the seater that the sense that respectses\n",
            "The secure in the cannot in the was hath the venge of the fire\n",
            "And with his a commanded and and dead,\n",
            "I am the rest the law of the people the the life,\n",
            "But I am I make a part to such all the father been thoughts,\n",
            "Which is the fairest say is in hearing son.\n",
            "There should with his friends to thee with the sent to heaven,\n",
            "Because and more stood of the head,\n",
            "And the interce of the loved against the rest,\n",
            "The all be stood reached in my counted\n",
            "In promised the the face of the that shame time\n",
            "To kings to light the great sings wretch\n",
            "That he did bear the best death of the soul,\n",
            "Who know the duke in that hath we have from t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated = model.generate(contexts[0, :], 1000, 1)\n",
        "print(tokenizer.decode(generated.numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qVv6MjxVJA1",
        "outputId": "91ec374e-3825-4970-9d3a-3ad122afc27b"
      },
      "id": "4qVv6MjxVJA1",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All my father speaknd execution, what I\n",
            "Grlook to thee.\n",
            "\n",
            "ELBOW:\n",
            "Here comes heads,\n",
            "Will bed theset my soul; and your blood punish'd,\n",
            "When he nourage hide slired underfumerlesse dent and land.\n",
            "\n",
            "PAPULET:\n",
            "I shall see as though oes in the sacle:\n",
            "Anon, my master, which if then parks myself,\n",
            "That throw state fly piece we, I\n",
            "say; not I beseech in to approach, sir:\n",
            "But Gemio, my lord.\n",
            "\n",
            "MORD Richard,\n",
            "Whereight it for his life.\n",
            "\n",
            "LADY ANNE:\n",
            "O, not get, and one new shame; and\n",
            "behold her gage.\n",
            "\n",
            "CLARENCE:\n",
            "Though that down my lord?\n",
            "\n",
            "WARWICK:\n",
            "And was it servant, this mother\n",
            "Were to-morrow, singling held Bolingbroke\n",
            "Lenduct quick the lothrew of his child\n",
            "By you rainted now and Born? his by seen;\n",
            "The bury, we another of themies, swelt his ground,\n",
            "And proffer. I wister'd, have did\n",
            "certain'd the look may hungrying; I have none committed;\n",
            "They down either proud of reful crowning manulto'd death.\n",
            "\n",
            "ANGELO:\n",
            "To faith, thou, and there the outward posses on.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Harinal respect with king had on is let \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": ".venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}