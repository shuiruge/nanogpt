{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eRCpWuVeBogx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eRCpWuVeBogx",
    "outputId": "72eab9ac-9e4e-4448-d3f8-aab258e606f6"
   },
   "outputs": [],
   "source": [
    "# # On Google colab:\n",
    "\n",
    "# # Data\n",
    "# !mkdir data\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "# !mv input.txt data/shakespeare.txt\n",
    "\n",
    "# # Module\n",
    "# !pip install git+https://github.com/shuiruge/nanogpt.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25758ae2-c983-4c56-a1e8-8bdb4858f44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locally:\n",
    "import sys\n",
    "sys.path.append('../nanogpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "250aebf8-3872-4ac5-ae23-a6beed98703e",
   "metadata": {
    "id": "250aebf8-3872-4ac5-ae23-a6beed98703e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-17 19:53:42.426062: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Layer, Dense, LayerNormalization, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import AdamW\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.callbacks import EarlyStopping\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "# from nanogpt.utils import (\n",
    "from utils import (\n",
    "    CharacterTokenizer, LanguageModelDataGenerator, TokPosEmbedding, FeedForward,\n",
    "    MultiHeadWrapper, ResNetWrapper, luong_attention,\n",
    ")\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7f6f09f-948d-4430-90a2-ac20b2310ec5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7f6f09f-948d-4430-90a2-ac20b2310ec5",
    "outputId": "6df1579a-bb00-46ae-8aaa-d9ea5c207c59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "with open('data/shakespeare.txt', 'r') as f:\n",
    "    corpus = ''.join(f.readlines())\n",
    "print(corpus[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "563aecf0-7f80-4e8c-a130-57a8bcff1df0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "563aecf0-7f80-4e8c-a130-57a8bcff1df0",
    "outputId": "8fa93e9e-a4a9-45ce-d17b-e38018bf90e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = CharacterTokenizer(corpus)\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "252be36e-effd-4d54-93dc-bf9bf50aee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 64\n",
    "data = LanguageModelDataGenerator(tokenizer.encode(corpus))\n",
    "\n",
    "contexts = []\n",
    "targets = []\n",
    "context = None\n",
    "while True:\n",
    "    try:\n",
    "        next_context, _ = data(seq_len, False)\n",
    "        if context is None:\n",
    "            context = next_context\n",
    "            continue\n",
    "        contexts.append(context)\n",
    "        targets.append(next_context)\n",
    "        context = next_context\n",
    "    except StopIteration:\n",
    "        break\n",
    "contexts = np.stack(contexts).astype('int64')\n",
    "targets = np.stack(targets).astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24099cff-8787-407e-a831-bf74d35a0c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14,\n",
       "         43, 44, 53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,\n",
       "          1, 39, 52, 63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43,\n",
       "         39, 56,  1, 51, 43,  1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50],\n",
       "        [47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43,\n",
       "         44, 53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1,\n",
       "         39, 52, 63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39,\n",
       "         56,  1, 51, 43,  1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50],\n",
       "        [56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "         53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39,\n",
       "         52, 63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,\n",
       "          1, 51, 43,  1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10]]),\n",
       " array([[47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43,\n",
       "         44, 53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1,\n",
       "         39, 52, 63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39,\n",
       "         56,  1, 51, 43,  1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50],\n",
       "        [56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "         53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39,\n",
       "         52, 63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,\n",
       "          1, 51, 43,  1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10],\n",
       "        [57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44, 53,\n",
       "         56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52,\n",
       "         63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1,\n",
       "         51, 43,  1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts[:3], targets[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c494a087-80a2-46de-8f16-79f1ae6e7879",
   "metadata": {
    "id": "c494a087-80a2-46de-8f16-79f1ae6e7879"
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(Layer):\n",
    "    \"\"\"A standard version of self-attention for GPT.\n",
    "\n",
    "    Args:\n",
    "        num_heads: int\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.multihead = MultiHeadWrapper(self.num_heads)\n",
    "        self.get_query = Dense(self.model_dim)\n",
    "        self.get_key = Dense(self.model_dim)\n",
    "        self.get_value = Dense(self.model_dim)\n",
    "\n",
    "    def call(self, x, return_weights=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: tf.Tensor\n",
    "                Shape [..., seq_len, dim]\n",
    "            return_weights: bool\n",
    "                If return the attention weights. Defaults to False.\n",
    "\n",
    "        Returns: tf.Tensor or (tf.Tensor, tf.Tensor)\n",
    "            If return_weights is false, then return the output only, which has\n",
    "            the same shape and dtype as the x. Otherwise, return the output as\n",
    "            well as the attention weights, which has shape\n",
    "            [..., num_heads, seq_len, seq_len].\n",
    "        \"\"\"\n",
    "        query = self.get_query(x)\n",
    "        key = self.get_key(x)\n",
    "        value = self.get_value(x)\n",
    "\n",
    "        # Split the last dimension into multiple heads.\n",
    "        query = self.multihead.split_heads(query)\n",
    "        key = self.multihead.split_heads(key)\n",
    "        value = self.multihead.split_heads(value)\n",
    "\n",
    "        # Mask the self-communication.\n",
    "        seq_len = tf.shape(x)[-2]\n",
    "        # Like\n",
    "        # [[0., 1., 1., 1.],\n",
    "        #  [0., 0., 1., 1.],\n",
    "        #  [0., 0., 0., 1.],\n",
    "        #  [0., 0., 0., 0.]]\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones([seq_len, seq_len]), -1, 0)\n",
    "\n",
    "        # The communication is implemented by a Luong-style attention.\n",
    "        output, attention_weights = luong_attention(query, key, value, mask)\n",
    "\n",
    "        # Concatenate the heads together.\n",
    "        output = self.multihead.concat_heads(output)\n",
    "\n",
    "        # The ResNet trick\n",
    "        return (output, attention_weights) if return_weights else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e26b40f9-380f-4309-9e3a-37c8458ad0e8",
   "metadata": {
    "id": "e26b40f9-380f-4309-9e3a-37c8458ad0e8"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int\n",
    "    seq_len: int\n",
    "    embed_dim: int\n",
    "    model_dim: int\n",
    "    num_heads: int\n",
    "    ffd_hidden_units: List[int]\n",
    "    num_trans_blocks: int\n",
    "\n",
    "\n",
    "class NanoGPT(Model):\n",
    "\n",
    "    def __init__(self, cfg: GPTConfig, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.embedding_layer = TokPosEmbedding(\n",
    "            cfg.vocab_size, cfg.seq_len, cfg.embed_dim)\n",
    "\n",
    "        # The so-called transformer-blocks.\n",
    "        self.trans_blocks = []\n",
    "        for _ in range(cfg.num_trans_blocks):\n",
    "            self.trans_blocks.append(\n",
    "                ResNetWrapper(CausalSelfAttention(cfg.model_dim, cfg.num_heads))\n",
    "            )\n",
    "            self.trans_blocks.append(\n",
    "                ResNetWrapper(FeedForward(cfg.ffd_hidden_units, cfg.model_dim))\n",
    "            )\n",
    "\n",
    "        self.output_layer = Dense(cfg.vocab_size)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding_layer(x)\n",
    "        for layer in self.trans_blocks:\n",
    "            x = layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def generate(self, init_token_ids, num_new_tokens, T):\n",
    "        \"\"\"Generates new tokens from the initial.\n",
    "\n",
    "        The \"temperature\" T controls the randomness, as in the Boltzmann\n",
    "        distributions.\n",
    "\n",
    "        Args:\n",
    "            init_token_ids: List[int]\n",
    "            num_new_tokens: int\n",
    "            T: float\n",
    "\n",
    "        Returns: List[int]\n",
    "            It also includes the initial token-IDs. So, the length is the\n",
    "            `len(initial_token_ids) + \n",
    "        \"\"\"\n",
    "        init_token_ids = tf.convert_to_tensor(init_token_ids)\n",
    "\n",
    "        # Add batch_size for matching the input shape of `self.call`.\n",
    "        # [1, len(init_token_ids)]\n",
    "        token_ids = tf.expand_dims(init_token_ids, axis=0)\n",
    "\n",
    "        for _ in range(num_new_tokens):\n",
    "            # [1, seq_len, vocab_size]\n",
    "            logits = self(token_ids[:, -self.cfg.seq_len:])\n",
    "            # We only use the last sequence element for output.\n",
    "            # [1, vocab_size]\n",
    "            logits = logits[:, -1, :]\n",
    "            # [1, 1]\n",
    "            next_token_id = tf.random.categorical(logits/T, 1)\n",
    "            token_ids = tf.concat([token_ids, next_token_id], axis=1)\n",
    "\n",
    "        # Drop the batch_size\n",
    "        token_ids = tf.squeeze(token_ids, axis=0)\n",
    "        return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c963ba25-d689-48e5-a736-6ef1a38446ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c963ba25-d689-48e5-a736-6ef1a38446ba",
    "outputId": "f13adb6b-4e8e-4ea5-b8bb-b3b2aee64727"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"nano_gpt\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " tok_pos_embedding (TokPosE  multiple                  8256      \n",
      " mbedding)                                                       \n",
      "                                                                 \n",
      " trans_block (TransBlock)    multiple                  45824     \n",
      "                                                                 \n",
      " trans_block_1 (TransBlock)  multiple                  45824     \n",
      "                                                                 \n",
      " dense_10 (Dense)            multiple                  4225      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 104129 (406.75 KB)\n",
      "Trainable params: 104129 (406.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Andrej Karpathy's configuration.\n",
    "# See: https://www.youtube.com/watch?v=kCc8FmEb1nY&t=6068s, at 1:40:30.\n",
    "# This configuration will get a minimal validation loss about 1.48.\n",
    "# And this configuration is not for our vanilla GPT, but Andrej's nano GPT,\n",
    "# which is much more complicated than ours.\n",
    "# cfg = GPTConfig(tokenizer.vocab_size, seq_len,\n",
    "#                 embed_dim=384,\n",
    "#                 model_dim=384,\n",
    "#                 num_heads=6,\n",
    "#                 ffd_hidden_units=[4*384],\n",
    "#                 num_trans_blocks=6)\n",
    "\n",
    "# cfg = GPTConfig(tokenizer.vocab_size, seq_len,\n",
    "#                 embed_dim=64,\n",
    "#                 model_dim=64,\n",
    "#                 num_heads=4,\n",
    "#                 ffd_hidden_units=[4*64],\n",
    "#                 num_trans_blocks=4)\n",
    "\n",
    "# We try a much much smaller one.\n",
    "cfg = GPTConfig(tokenizer.vocab_size, seq_len,\n",
    "                embed_dim=64,\n",
    "                model_dim=64,\n",
    "                num_heads=4,\n",
    "                ffd_hidden_units=[4*64],\n",
    "                num_trans_blocks=2)\n",
    "\n",
    "model = NanoGPT(cfg)\n",
    "model.build([None, seq_len])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ed79d10-1ad1-4350-909d-44e2df6a28cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(contexts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "129f1cce-c536-452d-8cfe-3b2303de497f",
   "metadata": {
    "id": "129f1cce-c536-452d-8cfe-3b2303de497f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-17 19:53:55.974183: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 513943552 exceeds 10% of free system memory.\n",
      "2024-03-17 19:53:57.140134: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 513943552 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15685/15685 [==============================] - 1506s 95ms/step - loss: 1.5407 - val_loss: 1.6744\n",
      "Epoch 2/100\n",
      "15685/15685 [==============================] - 743s 47ms/step - loss: 1.3956 - val_loss: 1.6534\n",
      "Epoch 3/100\n",
      "15685/15685 [==============================] - 747s 48ms/step - loss: 1.3689 - val_loss: 1.6556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f3f86b5de10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=AdamW(),\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    ")\n",
    "model.fit(\n",
    "    x=contexts,\n",
    "    y=targets,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    # The epochs argument shall be as large as possible. And we control the\n",
    "    # true epochs by early-stopping.\n",
    "    epochs=100,\n",
    "    callbacks=[EarlyStopping()]\n",
    ")\n",
    "# For our smaller configuration, the training will overfit after epoch 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "gscxB8dwHaFv",
   "metadata": {
    "id": "gscxB8dwHaFv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l:\n",
      "Well, I did that are shore punishes, to the consul!\n",
      "\n",
      "CLARENCE:\n",
      "I will not be purgundy her and and am and the horse.\n",
      "\n",
      "FLORIZEL:\n",
      "And say the word with a bones of the world,\n",
      "But strees with the commons of the fault of father's daughter.\n",
      "\n",
      "KING RICHARD III:\n",
      "My lord, I let thee counterfeit were in grace\n",
      "That he like a long man of bend the strength was should\n",
      "In a to the earth. Thou art not beat intend\n",
      "And so his father had made the fear was are prove\n",
      "With this hours are a wash\n",
      "And the should be dea\n"
     ]
    }
   ],
   "source": [
    "generated = model.generate(contexts[0, :], 500, 0.5)\n",
    "print(tokenizer.decode(generated.numpy())[seq_len:])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
